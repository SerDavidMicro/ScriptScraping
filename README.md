````markdown
# ğŸ•·ï¸ Crawler Web Interactivo

Un crawler ligero en Node.js con **menÃº interactivo** para elegir la URL de destino y el formato de salida (`html` o `json`).  
Guarda cada pÃ¡gina interna en `pages_html/` o `pages_json/` segÃºn tu elecciÃ³n.

---

## ğŸš€ CaracterÃ­sticas

- ğŸ’¬ **MenÃº CLI interactivo**  
  1. Introduce la URL base (aÃ±ade `https://` si falta)  
  2. Elige el formato de salida: `html` o `json`  
- ğŸ”— **Recorrido recursivo** de todos los enlaces internos en el mismo dominio  
- â±ï¸ **Retraso configurable** entre peticiones (por defecto 500 ms)  
- ğŸ“‚ **Carpeta por formato**:  
  - `pages_html/` â†’ `.html`  
  - `pages_json/` â†’ `.json`  

---

## ğŸ“‹ Prerrequisitos

- **Node.js** â‰¥ v12 (se probÃ³ en v16+)  
- **npm** (incluido con Node.js)  

---

## âš™ï¸ InstalaciÃ³n

1. **Clona este repositorio**  
   ```bash
   git clone https://github.com/<tu-usuario>/interactive-web-crawler.git
   cd interactive-web-crawler
````

2. **Instala dependencias**

   ```bash
   npm install axios cheerio fs-extra
   ```

---

## â–¶ï¸ Uso

1. **Lanza el crawler**

   ```bash
   node crawler.js
   ```
2. **Responde al menÃº**

   ```
   === CRAWLER INTERACTIVO ===

   1) Introduce la URL base (ej: https://www.ejemplo.com/):
   2) Elige formato de salida ("html" o "json"):
   ```
3. **Espera** a que el script descargue y guarde todas las pÃ¡ginas internas.
4. **Revisa** la carpeta generada (`pages_html/` o `pages_json/`).

---

## ğŸ“‚ Estructura de carpetas

```
.
â”œâ”€â”€ crawler.js         # Script principal
â”œâ”€â”€ package.json       # DefiniciÃ³n de dependencias
â”œâ”€â”€ pages_html/        # (modo html) archivos .html
â””â”€â”€ pages_json/        # (modo json) archivos .json
```

---

## ğŸ”§ ConfiguraciÃ³n

* **REQUEST\_DELAY\_MS**
  Ajusta el retraso entre peticiones en `crawler.js` (por defecto 500 ms).
* **ExtracciÃ³n JSON**
  Edita la secciÃ³n de Cheerio en `crawl()` para capturar otros selectores o campos.

---

## ğŸš¨ Precauciones

* Respeta el `robots.txt` y los tÃ©rminos de servicio del sitio.
* No procesa JavaScript dinÃ¡mico ni â€œinfinite scrollâ€.
* Para sitios grandes, considera:

  * Limitar profundidad de crawl
  * Controlar concurrencia
  * AÃ±adir reintentos ante errores

---

## ğŸ“ Licencia

Este proyecto se distribuye bajo la **Licencia MIT**.
Â¡Feliz crawling! ğŸš€


*------------------------------------------------------*
````markdown
# ğŸ•·ï¸ Interactive Web Crawler

A lightweight, interactive Node.js crawler that lets you:

- ğŸ” **Choose a base URL** at runtime (auto-prepends `https://` if missing)  
- ğŸ›ï¸ **Select output format** on the fly: `html` or `json`  
- ğŸ”— **Recursively crawl** every internal link on the same domain  
- â±ï¸ **Respect a configurable delay** between requests (default: 500 ms)  
- ğŸ“ **Save pages** into format-specific folders:  
  - `pages_html/` â†’ raw `.html` files  
  - `pages_json/` â†’ structured `.json` files  

---

## ğŸ“‹ Prerequisites

- **Node.js** v12+ (tested on v16+)  
- **npm** (bundled with Node.js)  

---

## âš™ï¸ Installation

1. **Clone this repository**  
   ```bash
   git clone https://github.com/<your-username>/interactive-web-crawler.git
   cd interactive-web-crawler
````

2. **Install dependencies**

   ```bash
   npm install axios cheerio fs-extra
   ```

---

## â–¶ï¸ Usage

1. **Run the crawler**

   ```bash
   node crawler.js
   ```

2. **Follow the interactive prompts**

   ```
   === INTERACTIVE CRAWLER ===

   1) Enter the base URL (e.g. https://www.example.com/):
   2) Choose output format ("html" or "json"):
   ```

3. **Wait** for the crawler to download and save every internal page.

4. **Inspect** the output folder (`pages_html/` or `pages_json/`) for results.

---

## ğŸ“‚ Folder Structure

```
.
â”œâ”€â”€ crawler.js         # Main crawler script
â”œâ”€â”€ package.json       # Dependency manifest
â”œâ”€â”€ pages_html/        # (html mode) raw .html files
â””â”€â”€ pages_json/        # (json mode) structured .json files
```

---

## ğŸ”§ Configuration

* **REQUEST\_DELAY\_MS**
  Adjust the delay between requests (default is 500 ms) in `crawler.js`.

* **Data Extraction**
  Customize the Cheerio selectors inside the `crawl()` function to capture additional fields.

---

## ğŸš¨ Caveats & Tips

* âœ… Always obey the siteâ€™s **robots.txt** and **Terms of Service**.
* ğŸš« This crawler does **not** execute JavaScript or handle infinite-scroll pages.
* ğŸ”§ For large sites, consider adding:

  * **Max depth** limits
  * **Concurrency** control
  * **Retry logic** on failure

---

## ğŸ“ License

This project is released under the **MIT License**.
Happy crawling! ğŸš€

```
```


